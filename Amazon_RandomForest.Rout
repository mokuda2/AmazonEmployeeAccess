
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.3     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.3     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.0
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> library(vroom)

Attaching package: ‘vroom’

The following objects are masked from ‘package:readr’:

    as.col_spec, col_character, col_date, col_datetime, col_double,
    col_factor, col_guess, col_integer, col_logical, col_number,
    col_skip, col_time, cols, cols_condense, cols_only, date_names,
    date_names_lang, date_names_langs, default_locale, fwf_cols,
    fwf_empty, fwf_positions, fwf_widths, locale, output_column,
    problems, spec

> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ rsample      1.2.0
✔ dials        1.2.0     ✔ tune         1.1.2
✔ infer        1.0.5     ✔ workflows    1.1.3
✔ modeldata    1.2.0     ✔ workflowsets 1.0.1
✔ parsnip      1.1.1     ✔ yardstick    1.2.0
✔ recipes      1.0.8     
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ scales::discard() masks purrr::discard()
✖ dplyr::filter()   masks stats::filter()
✖ recipes::fixed()  masks stringr::fixed()
✖ dplyr::lag()      masks stats::lag()
✖ yardstick::spec() masks vroom::spec(), readr::spec()
✖ recipes::step()   masks stats::step()
• Use suppressPackageStartupMessages() to eliminate package startup messages
> library(embed)
> library(discrim)

Attaching package: ‘discrim’

The following object is masked from ‘package:dials’:

    smoothness

> library(naivebayes)
naivebayes 0.9.7 loaded
> library(kknn)
> 
> amazon_train <- vroom("train.csv")
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> amazon_train$ACTION <- factor(amazon_train$ACTION)
> amazon_train
# A tibble: 32,769 × 10
   ACTION RESOURCE MGR_ID ROLE_ROLLUP_1 ROLE_ROLLUP_2 ROLE_DEPTNAME ROLE_TITLE
   <fct>     <dbl>  <dbl>         <dbl>         <dbl>         <dbl>      <dbl>
 1 1         39353  85475        117961        118300        123472     117905
 2 1         17183   1540        117961        118343        123125     118536
 3 1         36724  14457        118219        118220        117884     117879
 4 1         36135   5396        117961        118343        119993     118321
 5 1         42680   5905        117929        117930        119569     119323
 6 0         45333  14561        117951        117952        118008     118568
 7 1         25993  17227        117961        118343        123476     118980
 8 1         19666   4209        117961        117969        118910     126820
 9 1         31246    783        117961        118413        120584     128230
10 1         78766  56683        118079        118080        117878     117879
# ℹ 32,759 more rows
# ℹ 3 more variables: ROLE_FAMILY_DESC <dbl>, ROLE_FAMILY <dbl>,
#   ROLE_CODE <dbl>
> 
> amazon_test <- vroom("test.csv")
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> amazon_test
# A tibble: 58,921 × 10
      id RESOURCE MGR_ID ROLE_ROLLUP_1 ROLE_ROLLUP_2 ROLE_DEPTNAME ROLE_TITLE
   <dbl>    <dbl>  <dbl>         <dbl>         <dbl>         <dbl>      <dbl>
 1     1    78766  72734        118079        118080        117878     117879
 2     2    40644   4378        117961        118327        118507     118863
 3     3    75443   2395        117961        118300        119488     118172
 4     4    43219  19986        117961        118225        118403     120773
 5     5    42093  50015        117961        118343        119598     118422
 6     6    44722   1755        117961        117962        119223     125793
 7     7    75834  21135        117961        118343        123494     118054
 8     8     4675   3077        117961        118300        120312     124194
 9     9    18072  15575        117902        118041        118623     280788
10    10    22680   4474        117961        118446        119064     118321
# ℹ 58,911 more rows
# ℹ 3 more variables: ROLE_FAMILY_DESC <dbl>, ROLE_FAMILY <dbl>,
#   ROLE_CODE <dbl>
> 
> rf_model <- rand_forest(mtry = tune(),
+                         min_n = tune(),
+                         trees=1000) %>%
+   set_engine("ranger") %>%
+   set_mode("classification")
> 
> # Create a workflow with model & recipe
> target_encoding_amazon_recipe <- recipe(ACTION~., data=amazon_train) %>%
+   step_mutate_at(all_numeric_predictors(), fn=factor) %>%
+   step_lencode_mixed(all_nominal_predictors(), outcome=vars(ACTION))
> prep <- prep(target_encoding_amazon_recipe)
> baked_train <- bake(prep, new_data = amazon_train)
> 
> amazon_workflow <- workflow() %>%
+   add_recipe(target_encoding_amazon_recipe) %>%
+   add_model(rf_model)
> 
> # Set up grid of tuning values
> tuning_grid <- grid_regular(mtry(range=c(1,(ncol(amazon_train) - 1))),
+                             min_n(),
+                             levels = 10) ## L^2 total tuning possibilities
> 
> # Set up K-fold CV
> folds <- vfold_cv(amazon_train, v = 15, repeats=1)
> 
> CV_results <- amazon_workflow %>%
+   tune_grid(resamples=folds,
+             grid=tuning_grid,
+             metrics=metric_set(roc_auc)) #Or leave metrics NULL
→ A | warning: Model failed to converge with max|grad| = 0.353968 (tol = 0.002, component 1), Model is nearly unidentifiable: very large eigenvalue
                - Rescale variables?
There were issues with some computations   A: x1
→ B | warning: Model failed to converge with max|grad| = 0.349998 (tol = 0.002, component 1), Model is nearly unidentifiable: very large eigenvalue
                - Rescale variables?
There were issues with some computations   A: x1There were issues with some computations   A: x1   B: x1
→ C | warning: Model failed to converge with max|grad| = 0.352561 (tol = 0.002, component 1), Model is nearly unidentifiable: very large eigenvalue
                - Rescale variables?
There were issues with some computations   A: x1   B: x1There were issues with some computations   A: x1   B: x1   C: x1
→ D | warning: Model failed to converge with max|grad| = 0.352905 (tol = 0.002, component 1), Model is nearly unidentifiable: very large eigenvalue
                - Rescale variables?
There were issues with some computations   A: x1   B: x1   C: x1There were issues with some computations   A: x1   B: x1   C: x1   D: x1
→ E | warning: Model failed to converge with max|grad| = 0.347154 (tol = 0.002, component 1), Model is nearly unidentifiable: very large eigenvalue
                - Rescale variables?
There were issues with some computations   A: x1   B: x1   C: x1   D: x1There were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x1
→ F | warning: Model failed to converge with max|grad| = 0.348123 (tol = 0.002, component 1), Model is nearly unidentifiable: very large eigenvalue
                - Rescale variables?
There were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x1There were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…
→ G | warning: Model failed to converge with max|grad| = 0.351331 (tol = 0.002, component 1), Model is nearly unidentifiable: very large eigenvalue
                - Rescale variables?
There were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…There were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…
There were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…

> 
> # Find best tuning parameters
> bestTune <- CV_results %>%
+   select_best("roc_auc")
> 
> # Finalize workflow and predict
> final_wf <- amazon_workflow %>%
+   finalize_workflow(bestTune) %>%
+   fit(data=amazon_train)
> 
> amazon_predictions <- final_wf %>%
+   predict(new_data = amazon_test, type="prob")
> 
> amazon_predictions$Action <- amazon_predictions$.pred_1
> amazon_predictions$Id <- amazon_test$id
> amazon_final <- amazon_predictions %>%
+   select(c(Id, Action))
> 
> write.csv(amazon_final, "rfclassification.csv", row.names = F)
> 
> # library(tidyverse)
> # library(vroom)
> # library(tidymodels)
> # library(xgboost)  # Add XGBoost library
> # library(embed)
> # library(discrim)
> # library(naivebayes)
> # library(kknn)
> # 
> # # Load data
> # amazon_train <- vroom("train.csv")
> # amazon_train$ACTION <- factor(amazon_train$ACTION)
> # 
> # amazon_test <- vroom("test.csv")
> # 
> # # Define XGBoost model
> # xgb_model <- boost_tree(
> #   trees = tune(),
> #   tree_depth = tune(),
> #   mtry = tune(),
> #   learn_rate = tune(),
> #   loss_reduction = tune(),
> #   sample_size = tune(),
> #   stop_iter = 50  # Define a stopping criterion
> # ) %>%
> #   set_engine("xgboost", objective = "binary:logistic") %>%
> #   set_mode("classification")
> # 
> # # Create a workflow with model & recipe
> # target_encoding_amazon_recipe <- recipe(ACTION ~ ., data = amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
> #   step_normalize()
> # prep <- prep(target_encoding_amazon_recipe)
> # baked_train <- bake(prep, new_data = amazon_train)
> # 
> # amazon_workflow <- workflow() %>%
> #   add_recipe(target_encoding_amazon_recipe) %>%
> #   add_model(xgb_model)
> # 
> # # Set up grid of tuning values
> # tuning_grid <- grid_regular(
> #   trees(range = c(400, 500)),  # Adjust the range as per your preference
> #   tree_depth(range = c(3, 10)),
> #   mtry(range = c(1, (ncol(amazon_train) - 1))),
> #   learn_rate(range = c(0.01, 0.1)),
> #   loss_reduction(range = c(0, 1)),
> #   sample_size(range = c(0, 1))
> # )
> # 
> # # Set up K-fold CV
> # folds <- vfold_cv(amazon_train, v = 10, repeats = 1)
> # 
> # CV_results <- amazon_workflow %>%
> #   tune_grid(resamples = folds, grid = tuning_grid, metrics = metric_set(roc_auc))
> # 
> # # Find best tuning parameters
> # bestTune <- CV_results %>%
> #   select_best("roc_auc")
> # 
> # # Finalize workflow and predict
> # final_wf <- amazon_workflow %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data = amazon_train)
> # 
> # amazon_predictions <- final_wf %>%
> #   predict(new_data = amazon_test, type = "prob")
> # 
> # amazon_predictions$Action <- amazon_predictions$.pred_1
> # amazon_predictions$Id <- amazon_test$id
> # amazon_final <- amazon_predictions %>%
> #   select(c(Id, Action))
> # 
> # write.csv(amazon_final, "xgboost_classification.csv", row.names = FALSE)
> 
> proc.time()
     user    system   elapsed 
22247.685   409.983 21028.098 
